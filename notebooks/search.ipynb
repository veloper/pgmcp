{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2824bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload modules when code changes\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1caac9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmcp.settings import get_settings\n",
    "\n",
    "SETTINGS = get_settings()\n",
    "from rich.table import Table\n",
    "from rich.console import Console\n",
    "from rich.theme import Theme\n",
    "\n",
    "\n",
    "console = Console(highlight=False, force_jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "242db286",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pgmcp.models.library import Library\n",
    "\n",
    "\n",
    "KNOWLEDGE_BASE_LIBRARY_NAME = \"Knowledge Base\"\n",
    "_kb_library: Library | None = None\n",
    "async def get_knowledge_base_library() -> Library:\n",
    "    \"\"\"Get or create the knowledge base library.\"\"\"\n",
    "    \n",
    "    global _kb_library\n",
    "    if not _kb_library:\n",
    "        async with Library.async_context():\n",
    "            _kb_library = await Library.query().where(Library.name == KNOWLEDGE_BASE_LIBRARY_NAME).first()\n",
    "            if not _kb_library:\n",
    "                _kb_library = Library(name=KNOWLEDGE_BASE_LIBRARY_NAME)\n",
    "                await _kb_library.save()\n",
    "    return _kb_library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6397e579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy.future import select\n",
    "from pgmcp.models import Corpus, Document, Chunk \n",
    "import openai\n",
    "\n",
    "QUERY = \"on save callbacks\"\n",
    "\n",
    "async with Corpus.async_context():\n",
    "    library = await get_knowledge_base_library()\n",
    "\n",
    "    # 2. We need to embed the query\n",
    "    from openai import AsyncOpenAI\n",
    "    client = AsyncOpenAI()\n",
    "\n",
    "    response = await client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=QUERY\n",
    "    )\n",
    "\n",
    "    if not response or not response.data or not isinstance(response.data, list):\n",
    "        raise ValueError(f\"Invalid response from OpenAI: {response}\")\n",
    "\n",
    "    query_embedding = response.data[0].embedding    \n",
    "    \n",
    "    if not query_embedding or not isinstance(query_embedding, list):\n",
    "        raise ValueError(f\"Invalid embedding in response: {response.data[0]}\")\n",
    "\n",
    "QUERY_EMBEDDING = query_embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f40b43",
   "metadata": {},
   "source": [
    "# Vector Similarity\n",
    "\n",
    "## The Metaphor: Archery Exhibition\n",
    "\n",
    "### Setting\n",
    "\n",
    "- Holodeck: A simulated, multi-dimensional environment.\n",
    "- Gravity: None; Arrows travel in straight lines.\n",
    "- Air Resistance: Present; More draw on the bowstring results in greater distance traveled in space.\n",
    "\n",
    "### Vocabulary\n",
    "\n",
    "- **Vector** -- The set of coordinates where the arrow lands after you shoot it. For example, (x₁, x₂, ..., xₙ).\n",
    "- **Origin** -- The starting point (0, 0, ..., 0) where every shot begins.\n",
    "- **Distance (Magnitude)** -- The straight-line length from the origin to where the arrow lands. This is just the Pythagorean theorem, extended to as many dimensions as you have:\n",
    "    - In 2D: `distance = math.sqrt(a**2 + b**2)`\n",
    "        - Classic `a² + b² = c²`\n",
    "    - In nD: `distance = math.sqrt(sum(x**2 for x in vector))`\n",
    "        - Replace `c` with `DISTANCE`, so `a² + b² = DISTANCE²`, allowing `SUM([xₙ²,...]) == DISTANCE²`\n",
    "    - Each coordinate is like a \"side\" in its own dimension. The formula always gives you the shortest path from the origin to the landing point, no matter how many dimensions you have!\n",
    "- **Direction** -- The \"way\" the arrow points from the origin, no matter how far it goes. In programming terms, you get the direction by dividing each coordinate by the distance (magnitude). This gives you a \"unit vector\" that always has length 1, but points in the same direction as the original arrow.\n",
    "\n",
    "    ```python\n",
    "    # Example: get direction for vector v\n",
    "    v = [x1, x2, ..., xn]\n",
    "    magnitude = math.sqrt(sum(x**2 for x in v))\n",
    "    direction = [x / magnitude for x in v]\n",
    "    ```\n",
    "### pgvector Comparators in metaphorical context.\n",
    "\n",
    "Setup: Two archers show off their trick \"shots\" and they get recorded as vectors.\n",
    "\n",
    "We can now compare those archers' shots using a few different pgvector comparators.\n",
    "\n",
    "- **l2_distance**:\n",
    "    - Measures the straight-line distance between where two arrows landed, just like using a ruler in multi-dimensional space.\n",
    "    - Answers \"how close did these two shots land to each other?\"—ignoring the path, only caring about the shortest possible gap.\n",
    "    - Use this when you want to find the most similar or nearest shots, regardless of direction or how each got there.\n",
    "- **max_inner_product**:\n",
    "    - Measures how much two arrows point in the same direction and how far they both traveled—combining direction and magnitude.\n",
    "    - Useful for ranking which arrows (vectors) are most \"aligned\" and powerful compared to a reference shot.\n",
    "    - Great when you want to find shots that not only aim the same way, but also have the most \"force\" behind them (largest combined effect).\n",
    "- **cosine_distance**:\n",
    "    - Measures how closely two arrows point in the same direction, completely ignoring how far they traveled.\n",
    "    - Calculated as the dot product of the two vectors divided by the product of their magnitudes: `cosine_distance = dot(a, b) / (||a|| * ||b||)`.\n",
    "    - Use this when you want to find arrows (vectors) that are aimed the same way, even if they landed at very different distances from the origin—perfect for matching \"intent\" or \"approach\" rather than exact landing spots.\n",
    "- **l1_distance**:\n",
    "    - Adds up the differences between each coordinate of two arrows—like counting the total number of steps you'd take moving along a grid to match one shot to another.\n",
    "    - Use this when you care about the total adjustment needed in every direction, not just the straight-line distance—great for comparing feature-by-feature changes or when every axis matters.\n",
    "    - Ideal if you want to know \"how much work\" it would take to transform one shot into another by moving only along the axes, not diagonally.\n",
    "- **hamming_distance**:\n",
    "    - Counts how many coordinates are different between two arrows—like tallying up how many settings you changed on your bow between shots.\n",
    "    - Best for comparing arrows (vectors) in a space where each coordinate is a discrete choice (like on/off, yes/no, or categories).\n",
    "    - Use this when you care about the number of differences, not how big those differences are—great for error detection or comparing categorical data.\n",
    "- **jaccard_distance**:\n",
    "    - Measures how different two sets of coordinates are by comparing what they have in common versus what they have in total.\n",
    "    - Think of it as checking how much overlap there is between two arrows' landing spots—perfect overlap means they're identical, no overlap means they're totally different.\n",
    "    - Use this when your arrows are defined by sets of features or categories, and you want to know how much their features overlap.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3b4510",
   "metadata": {},
   "source": [
    "## Idea\n",
    "\n",
    "**LLM Routing to choose best approach to use based on user query intent.**\n",
    "\n",
    "It would be highly beneficial to have an LLM choose the most appropriate similarity metric based on the user’s query intent.\n",
    "\n",
    "- Different user questions naturally map to different notions of “similarity”—an LLM can infer this intent and select the optimal metric, improving result relevance.\n",
    "- This approach lets you support a wider range of search behaviors (e.g., intent-matching, feature-matching, nearest-neighbor) without forcing users to know or care about the underlying math.\n",
    "- It also enables more advanced workflows, like hybrid or fallback strategies (e.g., try cosine, then l2 if no strong matches), all orchestrated by the LLM.\n",
    "- In short: letting an LLM dynamically select the comparator makes your search system smarter, more flexible, and more user-aligned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8de44f",
   "metadata": {},
   "source": [
    "## LLM Decision Matrix Prompt: Choosing the Best pgvector Comparator for Knowledge Base Search\n",
    "\n",
    "You are an expert search assistant. Your task is to select the most appropriate pgvector similarity metric for searching a documentation knowledge base, based on the user's query and intent. \n",
    "\n",
    "## Available Comparators\n",
    "\n",
    "| Comparator           | Description |\n",
    "|----------------------|-------------|\n",
    "| **l2_distance**      | Use when the user wants the most similar or nearest documentation, regardless of approach or context. Best for \"find the closest match\" scenarios. |\n",
    "| **max_inner_product**| Use when the user wants results that are both highly relevant and information-rich—prioritize docs that are strongly aligned and comprehensive. |\n",
    "| **cosine_distance**  | Use when the user cares about matching the intent or conceptual direction of their query, even if the amount of detail differs. Best for \"find docs with the same approach or intent.\" |\n",
    "| **l1_distance**      | Use when the user wants to minimize total differences across all features or aspects—great for \"feature-by-feature\" or \"stepwise\" similarity. |\n",
    "| **hamming_distance** | Use when the user cares about the number of exact differences in discrete or categorical features (e.g., toggles, flags, categories). |\n",
    "| **jaccard_distance** | Use when the user wants to maximize overlap in sets of features, tags, or categories—best for set-based or tag-based matching. |\n",
    "\n",
    "## Instructions\n",
    "1. Analyze the user's query and infer their intent (e.g., are they seeking the closest match, conceptual alignment, feature overlap, etc.) using a chain of thought that considers the specific wording and context of the query. It should be between 3 and 10 connected thoughts long.\n",
    "2. Select the single most appropriate pgvector comparator from the table above.\n",
    "3. Response must pass validation against the JSON schema provided in the Response Schema section, and may not contain any but valid and parsable JSON.\n",
    "\n",
    "## Response Schema\n",
    "```json\n",
    "{\n",
    "  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "  \"title\": \"Pgvector Comparator Selection Process\",\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"a__chain_of_thought\": {\n",
    "      \"type\": \"array\",\n",
    "      \"description\": \"A step-by-step chain of thought (3-10 connected thoughts) showing how the comparator was selected.\",\n",
    "      \"minItems\": 3,\n",
    "      \"maxItems\": 10,\n",
    "      \"items\": {\n",
    "        \"type\": \"string\"\n",
    "      }\n",
    "    }\n",
    "    \"b__reason\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"A one-sentence justification for why this comparator was chosen, based on the chain_of_thought property array thinking\"\n",
    "    },\n",
    "    \"c__comparator\": {\n",
    "      \"type\": \"string\",\n",
    "      \"enum\": [\n",
    "        \"l2_distance\",\n",
    "        \"max_inner_product\",\n",
    "        \"cosine_distance\",\n",
    "        \"l1_distance\",\n",
    "        \"hamming_distance\",\n",
    "        \"jaccard_distance\"\n",
    "      ],\n",
    "      \"description\": \"The selected pgvector comparator for the search, chosen after using chain_of_thought and reason to justify its use.\"\n",
    "    },\n",
    "  },\n",
    "  \"required\": [\"a__chain_of_thought\", \"b__reason\", \"c__comparator\"],\n",
    "  ```\n",
    "  \n",
    "\n",
    "## Example Input / Output:**\n",
    "<examples>\n",
    "    <example>\n",
    "        <conversation>\n",
    "            <user>Find documentation on how to use the new API features of SqlAlchemy2x</user>\n",
    "            <assistant>{\n",
    "                \"a__chain_of_thought\": [\n",
    "                    \"The user is looking for documentation on a specific version of SqlAlchemy.\",\n",
    "                    \"They are interested in new API features, which suggests they want the latest information.\",\n",
    "                    \"The query is straightforward and does not imply a need for deep contextual understanding.\"\n",
    "                ],\n",
    "                \"b__reason\": \"The l2_distance comparator is best for finding the closest match without needing to understand the context deeply.\",\n",
    "                \"c__comparator\": \"l2_distance\"\n",
    "            }\n",
    "            </assistant>\n",
    "        </conversation>\n",
    "    </example>\n",
    "    <example>\n",
    "        <conversation>\n",
    "            <user>How do I implement pagination in FastAPI?</user>\n",
    "            <assistant>{\n",
    "                \"a__chain_of_thought\": [\n",
    "                    \"The user is looking for information on implementing pagination.\",\n",
    "                    \"They are using FastAPI, which suggests they want a solution that fits within that framework.\",\n",
    "                    \"The query is specific and likely requires examples or detailed explanations.\",\n",
    "                    \"The user may also want an idiomatic solution that follows FastAPI best practices.\"\n",
    "                ],\n",
    "                \"b__reason\": \"The max_inner_product comparator is best for finding highly relevant and information-rich documentation.\",\n",
    "                \"c__comparator\": \"max_inner_product\"\n",
    "            }\n",
    "            </assistant>\n",
    "        </conversation>\n",
    "    </example>\n",
    "</examples>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "676aae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search the postgresql database using similarity search with pgvector\n",
    "\n",
    "\"\"\"\n",
    "pgvector offers l2_distance, max_inner_product, cosine_distance, l1_distance, hamming_distance, and jaccard_distance\n",
    "\n",
    "## Metaphor Time:\n",
    "\n",
    "### Premise: \n",
    "- You are an Archer, standing at `origin` of the entire vector space.\n",
    "- You fire an arrow wildly into multi-dimensional space.\n",
    "- The direction and distance of that shot if your `reference` from which you will \n",
    "  _measure_ how close or distant it was to other historical paths of the _shot_.\n",
    "- You can think of each shot as a vector in this space, with its own unique direction and magnitude.\n",
    "    - Direction: The angle at which the arrow was shot.\n",
    "    - Magnitude: The distance the arrow traveled.\n",
    "- We don't care where the shot originated at all -- we only care about its direction (angle shot at) and magnitude (distance traveled).\n",
    "- With those two pieces of information we can then compare _past_ shots to any other shot, and from that we can likely determine how similar they are.\n",
    "- That covers shot similarity. But, we also know what it takes to \"hit\" a `target` which is defined by its own unique set of coordinates in this multi-dimensional space.\n",
    "- We can tell how \"close\" any given shot it to a `target` by comparing a shot's vector to the target's vector. The closer they are in direction and magnitude, the more likely the shot is to hit the target.\n",
    "- We can actually factor out distance and just compare `direction` (angle of the shot taken in multi-dimensional space) and the angle of the target's vector in multi-dimensional space. (from the theoretical origin of the multi-dimensional space)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pgmcp.models.chunk import Chunk\n",
    "from pgmcp.models.document import Document\n",
    "\n",
    "results = []\n",
    "async with Chunk.async_context() as session:\n",
    "    qb = Chunk.query()\n",
    "    qb = qb.joins(Chunk.document)\n",
    "    qb = qb.where(Chunk.embedding.l2_distance(query_embedding) < 0.5)\n",
    "\n",
    "# Search all chunks in the corpus using similarity search with pgvector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
